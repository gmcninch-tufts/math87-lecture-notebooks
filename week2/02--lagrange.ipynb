{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Math087 - Mathematical Modeling\n",
    "===============================\n",
    "[Tufts University](http://www.tufts.edu) -- [Department of Math](http://math.tufts.edu)  \n",
    "[George McNinch](http://gmcninch.math.tufts.edu) <george.mcninch@tufts.edu>  \n",
    "*Fall 2020*\n",
    "\n",
    "Course material (Week 2): Lagrange multpliers \n",
    "---------------------------------------------\n",
    "\n",
    "(multivariable optimization, continued)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some motivation\n",
    "----------------\n",
    "Let's recall our television manufacturing example for a moment. The model we solved was interesting \n",
    "but most likely unrealistic. A manufacturer, for instance, probably has a limited\n",
    "capacity and can only produce a certain amount of TVs in total per year. \n",
    "\n",
    "Let’s look at an example where that capacity is 10,000 TVs per year.\n",
    "\n",
    "So given the *constraint*, $s + t ≤ 10,000$, how many of each model should they produce now??\n",
    "\n",
    "Well, first notice that the *unconstrained optimum* ofwith $s = 4,735$ and $t = 7,043$,\n",
    "does not satisfy the constraint.\n",
    "\n",
    "Now let's recall that in the single variable case, trying to find a *constrained optimum* amounts t optimizing a function on a closed interval -- so you proceed with the usual procedure for optimization but must also check boundary points (endpoints).\n",
    "\n",
    "If we proceed in this fashion, we'd check the \"boundary\" conditions corresponding to $t=0$ and $s=0$.\n",
    "\n",
    "Well, setting $t=0$ we see that the profit function is given by\n",
    "\n",
    "$$P(s,0) = 144s - 0.01s^2 - 400,000.$$\n",
    "\n",
    "Since this is a function only of $s$ we can use 1-dimensional optimization techniques:\n",
    "\n",
    "$$\\dfrac{\\partial P}{\\partial S}(S,0) = 144 - 0.02s = 0 \\implies s = 7,200.$$\n",
    "\n",
    "On this boundary, we need to consider $(0,0)$ and $(10,000,0)$ (the boundary of the boundary...) as well as $(7,200,0)$.\n",
    "\n",
    "We can treat the boundary with $s =0$ similarly.\n",
    "\n",
    "But the boundary condition with $s+t = 0$ will be more of a pain.\n",
    "\n",
    "The method of Lagrange multipliers gives a more systematic way to proceed!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lagrange multipliers\n",
    "\n",
    "Consider a function $f(x,y)$ of two variables. We are interested here in finding maximal or minimal values of $f$ subject to a *constraint*. The sort of constraint we have in mind is a restriction on the possible pairs $(x,y)$ -- so we have a second function $g(x,y)$ and we want to maximize (or minimize) $f$ subject\n",
    "to the condition that $g(x,y) = c$ for some fixed quantity $c$.\n",
    "\n",
    "We introduce a \"new\" function -- now of *three* variables - known as the **Lagrangian**. It is given by the formula\n",
    "$$F(x,y,\\lambda) = f(x,y) - \\lambda \\cdot (c-g(x,y))$$\n",
    "\n",
    "(We use the Greek letter $\\lambda$ for our third variable in part because it plays a different role for us than the variables $x,y$).\n",
    "\n",
    "We can calculate the *partial derivatives* of this Lagrangian; they are:\n",
    "\n",
    "$$\\dfrac{\\partial F}{\\partial x} = \\dfrac{\\partial f}{\\partial x} - \\lambda\\dfrac{\\partial g}{\\partial x}$$\n",
    "\n",
    "$$\\dfrac{\\partial F}{\\partial y} = \\dfrac{\\partial f}{\\partial y} - \\lambda\\dfrac{\\partial g}{\\partial y}$$\n",
    "\n",
    "$$\\dfrac{\\partial F}{\\partial \\lambda} = c-g(x,y)$$\n",
    "\n",
    "If we seek critical points of the Lagrangian, we find that \n",
    "$$0 = \\dfrac{\\partial F}{\\partial x} = \\dfrac{\\partial f}{\\partial x} - \\lambda\\dfrac{\\partial g}{\\partial x}$$\n",
    "and similarly for $y$, so that\n",
    "$$ \\dfrac{\\partial f}{\\partial x} = \\lambda \\dfrac{\\partial g}{\\partial x} \\quad \\text{and}\\quad\n",
    "\\dfrac{\\partial f}{\\partial y} = \\lambda \\dfrac{\\partial g}{\\partial y}$$\n",
    "i.e.\n",
    "$$ \\left (\\dfrac{\\partial f}{\\partial x} \\mathbf{i} + \\dfrac{\\partial f}{\\partial y} \\mathbf{j} \\right)\n",
    "= \\lambda \\left (\\dfrac{\\partial g}{\\partial x} \\mathbf{i} + \\dfrac{\\partial g}{\\partial y} \\mathbf{j} \\right)$$\n",
    "\n",
    "(Recall that $\\dfrac{\\partial f}{\\partial x} \\mathbf{i} + \\dfrac{\\partial f}{\\partial y} \\mathbf{j}$ is the\n",
    "*gradient* $\\nabla f$ of $f$).\n",
    "\n",
    "Moreover, we find that\n",
    "$$0 = \\dfrac{\\partial F}{\\partial \\lambda} = c - g(x,y).$$\n",
    "\n",
    "Summarizing, the condition that $(x_0,y_0,\\lambda_0)$ is a critical point of $F$ is equivalent to two requirements: \n",
    "- **(A)** $(x_0,y_0)$ must be on the level curve $g(x,y) = c$, and\n",
    "- **(B)** the gradient vectors must satisfy $\\nabla f \\vert_{(x_0,y_0)} = \\lambda_0 \\nabla g \\vert_{(x_0,y_0)}$.\n",
    "\n",
    "The crucial assertion is this:\n",
    "------------------------------\n",
    "\n",
    "> Optimal values for $f$ along the level curve $g(x,y) = c$ will be found among the critical points of $F$. \n",
    "\n",
    "Indeed, suppose $(x_0,y_0)$\n",
    "is a point on the level curve at which $f$ takes its max (or min) value (on the level surface).\n",
    "We need to argue that the gradient vector $\\nabla f \\vert_{(x_0,y_0)}$ is \"parallel\" to the gradient\n",
    "vector $\\nabla g \\vert_{(x_0,y_0)}$ -- i.e. **(B)** above holds.\n",
    "\n",
    "More precisely, we can write $\\nabla f \\vert_{(x_0,y_0)} = \\mathbf{v} + \\mu \\nabla g \\vert_{(x_0,y_0)}$\n",
    "for a vector $\\mathbf{v}$ perpendicular to $\\nabla g \\vert_{(x_0,y_0)}$ (and for some scalar $\\mu$).\n",
    "And we must argue that $\\mathbf{v}$ is zero.\n",
    "\n",
    "But if $\\mathbf{v}$ is non-zero, then walking along the level curve $g(x,y) = c$ \"in the direction of $\\mathbf{v}$\" \n",
    "will lead to higher values of $f$, contrary to the assumption that on the level curve, $f$ has a maximum at $(x_0,y_0)$. \n",
    "$\\quad \\blacksquare$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Televisions, again\n",
    "-------------------\n",
    "\n",
    "Let’s return to the television manufacturing problem. We consider the constraint $$g(s, t) = s + t = 10,000$$\n",
    "i.e. \"the manufacturer produces exactly 10,000 televisions\".\n",
    "\n",
    "Consider the *Lagrangian* function $F(s,t,\\lambda) = P(s,t) - \\lambda(s+t-10,000).$\n",
    "\n",
    "Looking for critical points of $F$, we find that:\n",
    "\n",
    "$$ \\left \\{ \n",
    "\\begin{matrix}\n",
    "\\dfrac{\\partial P}{\\partial s}  - \\lambda \\dfrac{\\partial g}{\\partial s} &=& 144 − 0.02s − 0.007t − λ &= 0 \\\\\n",
    "\\dfrac{\\partial P}{\\partial t}  - \\lambda \\dfrac{\\partial g}{\\partial t} &=&  174 − 0.007s − 0.002t − λ &= 0 \\\\\n",
    "g(s,t) - c &=& -10000 +s +t &=  0\n",
    "\\end{matrix}\n",
    "\\right .$$\n",
    "\n",
    "This leads to 3 linear equations in 3 unknowns, which we can easily solve with ``numpy`` (or by hand!): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3846.15384615, 6153.84615385,   24.        ])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "## coefficient matrix\n",
    "M=np.array([[0.02,.007,1],[.007,.02,1],[1,1,0]])\n",
    "\n",
    "b=np.array([144,174,10000])\n",
    "\n",
    "np.linalg.solve(M,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we find that\n",
    "$s \\approx 3,846$, $t \\approx 6,154$\n",
    "and $λ = 24$.\n",
    "\n",
    "\n",
    "\n",
    "Now, effectively we consider the profit function $P$ subject to the constraint\n",
    "$s+t=10,000$ where $s \\ge 0$ and $t \\ge 0$. On this \"closed interval\", the function $P$ will assume a maximum and a minimum value. We've found the \"critical point\" of $P$ on this \"interval\" -- namley $(3846, 6154)$. The *endpoints* of the interval are $(0,10000)$ and $(10000,0)$.\n",
    "\n",
    "\n",
    "Let's compare the values of $P$ at these points of interest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[340000.0, 532307.692, 40000.0]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def p(s,t):\n",
    "    return -400000 + 144*s + 174*t - 0.01*s**2 - 0.01*t**2 - .007*s*t\n",
    "\n",
    "[p(0,10000), p(3846,6154), p(10000,0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that the *maximum constrained profit* is\n",
    "$$P(3846 , 6154) = 532308.$$\n",
    "\n",
    "(what choice of $s,t$ give the minimum constrained profit? Could you have guessed that *a priori*?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shadow prices and an interpretation for the \"multiplier\" $\\lambda$\n",
    "-------------------------------------------------------------------\n",
    "\n",
    "Let's carry out *sensitivity analysis* on the value of the *constraint*.\n",
    "\n",
    "Recall that our constraint is $g(s,t) = s+t = 10000$. So we instead set\n",
    "$$g(s,t) = s + t = c.$$\n",
    "\n",
    "In this case, we must instead solve the system of equations\n",
    "$Mx = b$ where\n",
    "$$M = \\begin{pmatrix} 0.02 & 0.007 & 1 \\\\\n",
    "0.007 & 0.02 & 1 \\\\\n",
    "1 & 1 & 0 \n",
    "\\end{pmatrix} \\quad \\text{and} \\quad\n",
    "b = \\begin{pmatrix}\n",
    " 144 \\\\ 174 \\\\ c\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "\n",
    "Since the vector $b$ has an \"unknown\", we can't seem to use the ``linalg.solve`` method. We can circumvent this by inverting the coefficient matrix. We must also introduce a ``symbol`` for the unknown ``c``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5*c - 1153.84615384615, 0.5*c + 1153.84615384615,\n",
       "       159.0 - 0.0135*c], dtype=object)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sympy as sp\n",
    "\n",
    "## coefficient matrix\n",
    "M=np.array([[0.02,0.007,1],[0.007,0.02,1],[1,1,0]])\n",
    "\n",
    "## inverse of coefficient matrix\n",
    "Mi = np.linalg.inv(M)\n",
    "\n",
    "c = sp.Symbol('c')\n",
    "b=np.array([144,174,c])\n",
    "\n",
    "\n",
    "np.matmul(Mi,b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we see that the solution has\n",
    "$$s = \\dfrac{c}{2} - 1153.85 \\qquad t = \\dfrac{c}{2} + 1153.85 \\qquad \\text{and} \\qquad \\lambda = 159 \n",
    "- 0.0135c.$$\n",
    "\n",
    "Thus $\\dfrac{ds}{dc} = \\dfrac{dt}{dc} = \\dfrac{1}{2}$ so that\n",
    "$$S(s,c=10000) \\approx 1.3 \\quad S(t,c=10000) \\approx 0.8$$\n",
    "which shows that increasing the maximum production of TVs will increase the optimal production levels.\n",
    "\n",
    "What about the sensitivity $S(P,c) = \\dfrac{dP}{dc} \\cdot \\dfrac{c}{P}$?\n",
    "\n",
    "To compute $\\dfrac{dP}{dc}$ we can use the several-variable chain rule, or just rewrite\n",
    "$P$ as a function of $c$. After some calculation, we find that\n",
    "$$S(P,c=10000) \\approx 24 \\cdot \\dfrac{10000}{532308} \\approx 0.45.$$\n",
    "So a 1% increase in $c$ yields a $0.45%$ increase in $P$.\n",
    "\n",
    "Interestingly, observe that $$\\dfrac{dP}{dc} = \\lambda.$$\n",
    "\n",
    "Let's pause to recall that with the method of Lagrange multipliers, the value of gradient of the function we are optimizing -- $P$ in this case -- at a candidate optimal point is proportional to the value of the gradient of $g$:\n",
    "$$ \\nabla(P)_{(s_0,t_0)} = \\lambda \\nabla(g)_{(s_0,t_0)}$$\n",
    "\n",
    "One refers to $\\dfrac{\\partial P}{\\partial c}$ as the **shadow price** -- increasing $c$ by 1 unit in this case increases $P$ by about \\$24.\n",
    "\n",
    "Thus, the Lagrange multiplier, λ actually has a \"physical\" meaning here. If you are allowed to\n",
    "produce more TV’s it tells you how much that change affects your profit. Therefore, if the cost of\n",
    "making that change is less than the additional profit, you probably should go for it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
